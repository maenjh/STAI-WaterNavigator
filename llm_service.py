import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

MODEL_PATH = "/workspace/models/Meta-Llama-3-8B"
llm_pipeline = None

def load_model():
    """
    ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ Llama-3 ëª¨ë¸ì„ ë¡œë“œí•˜ê³  íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    4ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì í™”í•©ë‹ˆë‹¤.
    """
    global llm_pipeline
    if llm_pipeline is not None:
        print("âœ… ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    print("ğŸ§  Llama-3 ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

    llm_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.float16,
        max_new_tokens=1024,
        repetition_penalty=1.1,
    )
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

def get_recommendations(prompt: str) -> str:
    """
    ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ LLMì„ í†µí•´ ìˆ˜ì›ì§€ ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if llm_pipeline is None:
        raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € load_model()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")

    print(f"ğŸ’¬ ì¶”ì²œ ìƒì„± í”„ë¡¬í”„íŠ¸: {prompt[:100]}...") # ë¡œê·¸ì— í”„ë¡¬í”„íŠ¸ ì¼ë¶€ë§Œ ì¶œë ¥
    
    # íŒŒì´í”„ë¼ì¸ì— í† í¬ë‚˜ì´ì €ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì§ì ‘ ì ‘ê·¼ ê°€ëŠ¥
    eos_token_id = llm_pipeline.tokenizer.eos_token_id

    sequences = llm_pipeline(
        prompt,
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=eos_token_id,
    )
    
    if sequences and len(sequences) > 0:
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •
        return sequences[0]['generated_text'].replace(prompt, "").strip()
    return "" 